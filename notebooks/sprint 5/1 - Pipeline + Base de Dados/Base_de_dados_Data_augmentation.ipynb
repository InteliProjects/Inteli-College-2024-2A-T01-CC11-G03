{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install enelvo"
      ],
      "metadata": {
        "id": "OmJZg3XbGXAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-twBc2TRpYu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from enelvo.normaliser import Normaliser\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnoPT8gnRprN"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zKtGlTO_k37"
      },
      "source": [
        "# Pré-processamento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgQ368mGxOdd"
      },
      "source": [
        "## 0) Base de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsvYNwsYT2oa"
      },
      "outputs": [],
      "source": [
        "norm = Normaliser(tokenizer='readable')\n",
        "chats_clients = pd.read_csv('/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/data_sprint_3.csv')\n",
        "chats_clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCR35ieaUElp"
      },
      "outputs": [],
      "source": [
        "chats_clients.fillna('erro ao processar a pergunta', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def shuffle_words(sentence):\n",
        "    \"\"\"\n",
        "    Embaralha as palavras em uma frase\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    Remove as stopwords em uma frase\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    word_tokens = word_tokenize(sentence)\n",
        "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "    return ' '.join(filtered_sentence)\n",
        "\n",
        "# Aplicando o embaralhamento de palavras\n",
        "chats_clients['Pergunta_Shuffled'] = chats_clients['Pergunta'].apply(lambda x: shuffle_words(x))\n",
        "chats_clients['Resposta_Shuffled'] = chats_clients['Resposta'].apply(lambda x: shuffle_words(x))\n",
        "\n",
        "\n",
        "# Mostrar as primeiras linhas após as técnicas de data augmentation\n",
        "chats_clients[['Pergunta', 'Pergunta_Shuffled', 'Resposta', 'Resposta_Shuffled']].head()\n"
      ],
      "metadata": {
        "id": "dAzgPN0oDK_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Retorna os sinônimos de uma palavra\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word, lang='por'):\n",
        "        for lemma in syn.lemmas(lang='por'):\n",
        "            synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_replacement(sentence, n=2):\n",
        "    \"\"\"\n",
        "    Substitui aleatoriamente n palavras por seus sinônimos\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if len(get_synonyms(word)) > 0]))\n",
        "    random.shuffle(random_word_list)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Aplicando a substituição de sinônimos\n",
        "chats_clients['Pergunta_Augmented'] = chats_clients['Pergunta'].apply(lambda x: synonym_replacement(x))\n",
        "chats_clients['Resposta_Augmented'] = chats_clients['Resposta'].apply(lambda x: synonym_replacement(x))\n",
        "\n",
        "# Mostrar as primeiras linhas após a substituição de sinônimos\n",
        "chats_clients[['Pergunta', 'Pergunta_Augmented', 'Resposta', 'Resposta_Augmented']].head()\n"
      ],
      "metadata": {
        "id": "MvfKh23LDak9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_deletion(words, p=0.14):\n",
        "    \"\"\"\n",
        "    Remove com a probabilidade p as palavras de uma frase\n",
        "    \"\"\"\n",
        "    words = words.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        r = random.uniform(0, 1)\n",
        "        if r > p:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "# Aplicando a deleção aleatória\n",
        "chats_clients['Pergunta_Deletion'] = chats_clients['Pergunta'].apply(lambda x: random_deletion(x))\n",
        "\n",
        "# Mostrar as primeiras linhas após a deleção aleatória\n",
        "chats_clients[['Pergunta_Deletion']]"
      ],
      "metadata": {
        "id": "GVL9KmWEjuWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um novo DataFrame para armazenar os dados aumentados\n",
        "df_augmented = pd.DataFrame()\n",
        "\n",
        "# Adicionar as linhas originais\n",
        "df_augmented = chats_clients.copy()\n",
        "\n",
        "# Adicionar as linhas de perguntas e respostas embaralhadas\n",
        "df_augmented_extra1 = pd.DataFrame({\n",
        "    'No': chats_clients['No'],\n",
        "    'Intencao': chats_clients['Intencao'],\n",
        "    'Pergunta': chats_clients['Pergunta_Shuffled'],\n",
        "    'Resposta': chats_clients['Resposta_Shuffled']\n",
        "})\n",
        "\n",
        "df_augmented_extra2 = pd.DataFrame({\n",
        "    'No': chats_clients['No'],\n",
        "    'Intencao': chats_clients['Intencao'],\n",
        "    'Pergunta': chats_clients['Pergunta_Augmented'],\n",
        "    'Resposta': chats_clients['Resposta_Augmented']\n",
        "})\n",
        "\n",
        "df_augmented_extra3 = pd.DataFrame({\n",
        "    'No': chats_clients['No'],\n",
        "    'Intencao': chats_clients['Intencao'],\n",
        "    'Pergunta': chats_clients['Pergunta_Deletion'],\n",
        "    'Resposta': chats_clients['Resposta']\n",
        "})\n",
        "# Concatenar o DataFrame original com o DataFrame de data augmentation\n",
        "df_augmented = pd.concat([df_augmented, df_augmented_extra1, df_augmented_extra2, df_augmented_extra3], ignore_index=True)\n",
        "\n",
        "# Remover as colunas auxiliares de shuffle\n",
        "df_augmented = df_augmented[['No', 'Intencao', 'Pergunta', 'Resposta']]\n",
        "\n",
        "# Exibir as primeiras linhas do DataFrame final\n",
        "df_augmented.shape\n"
      ],
      "metadata": {
        "id": "jtfNAyweC_P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93457097-cfc3-4be4-e9bf-e4939615f6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5636, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Especificar o caminho onde o arquivo CSV será salvo\n",
        "output_file_path = '/content/drive/Shareddrives/grupo3moshi/augmented_data.csv'\n",
        "\n",
        "# Salvar o DataFrame aumentado como um arquivo CSV\n",
        "df_augmented.to_csv(output_file_path, index=False)\n"
      ],
      "metadata": {
        "id": "wttJU4qbF4SX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}