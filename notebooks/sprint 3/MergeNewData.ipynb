{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NStX89Ri7bz8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(text):\n",
        "    \"\"\"\n",
        "    Remove emojis de um texto.\n",
        "\n",
        "    Args:\n",
        "        text (str): O texto a ser processado.\n",
        "\n",
        "    Returns:\n",
        "        str: O texto sem emojis.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"\n",
        "            \"\\U0001F300-\\U0001F5FF\"\n",
        "            \"\\U0001F680-\\U0001F6FF\"\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"\n",
        "            \"\\U00002500-\\U00002BEF\"\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U000024C2-\\U0001F251\"\n",
        "            \"\\U0001f926-\\U0001f937\"\n",
        "            \"\\U00010000-\\U0010ffff\"\n",
        "            \"\\u200d\"\n",
        "            \"\\u2640-\\u2642\"\n",
        "            \"\\u2600-\\u2B55\"\n",
        "            \"\\u23cf\"\n",
        "            \"\\u23e9\"\n",
        "            \"\\u231a\"\n",
        "            \"\\u3030\"\n",
        "            \"\\ufe0f\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def standardize_columns(df):\n",
        "  \"\"\"\n",
        "  Padroniza as colunas de um DataFrame.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com as colunas padronizadas.\n",
        "  \"\"\"\n",
        "  if 'Resposta\\n' in df.columns:\n",
        "    df = df.rename(columns={'Resposta\\n': 'Resposta'})\n",
        "  if 'Unnamed: 0' in df.columns:\n",
        "    df = df.rename(columns={'Unnamed: 0': 'No'})\n",
        "  if 'Comentario do Inacio' in df.columns:\n",
        "    df = df.drop(columns=['Comentario do Inacio'])\n",
        "\n",
        "  df = df.dropna(subset=['Pergunta', 'Resposta'], how='all')\n",
        "  df = df.replace('\\n', ' ', regex=True)\n",
        "\n",
        "  df['Pergunta'] = df['Pergunta'].apply(remove_emojis)\n",
        "  df['Resposta'] = df['Resposta'].apply(remove_emojis)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Afid_YDLx9GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Conversas1.csv\"\n",
        "conv2 = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Conversas2.csv\"\n",
        "qa = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Q&A2.csv\"\n",
        "dictionary = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Dicionario.csv\"\n",
        "old_df = \"/content/drive/Shareddrives/grupo3moshi/chats_clients.csv\"\n",
        "\n",
        "\n",
        "# Padroniza as colunas para depois conseguir concatenar todos os dataframes\n",
        "conv1_df = standardize_columns(pd.read_csv(conv1))\n",
        "conv2_df = standardize_columns(pd.read_csv(conv2))\n",
        "qa_df = standardize_columns(pd.read_csv(qa, on_bad_lines='skip'))\n",
        "dictionary_df = pd.read_csv(dictionary)\n",
        "old_df = standardize_columns(pd.read_csv(old_df))"
      ],
      "metadata": {
        "id": "3itpHm3SGxm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continua o número do chat a partir do último do dataframe antigo\n",
        "qa_df['No'] = range(len(old_df) + 1, len(old_df) + 1 + len(qa_df))\n",
        "\n",
        "# Concatena o dataframe de QA e o da sprint 2\n",
        "combined_qa_df = pd.concat([old_df, qa_df], ignore_index=True)\n",
        "print(len(combined_qa_df))"
      ],
      "metadata": {
        "id": "0SdlsJ2rL-67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_questions_and_answers(df, no_start):\n",
        "  \"\"\"\n",
        "  Combina as respostas e perguntas em uma única coluna, de uma forma que tenha\n",
        "  o histórico do chat até aquele momento.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "      no_start (int): O número inicial para o No (número do chat).\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com as perguntas e respostas combinadas.\n",
        "  \"\"\"\n",
        "  df['No'] = df['No'].fillna(method='ffill')\n",
        "  df['Intencao'] = df['Intencao'].fillna(method='ffill')\n",
        "\n",
        "  unique_numbers = df['No'].unique()\n",
        "\n",
        "  # Para cada chat, junta as perguntas e respostas, criando um histórico da\n",
        "  # conversa até aquela pergunta\n",
        "  for number in unique_numbers:\n",
        "    full_string = \"\"\n",
        "    first_line = True\n",
        "    rows = df.loc[df['No'] == number, ['Pergunta', 'Resposta']]\n",
        "    for index, row in rows.iterrows():\n",
        "      if first_line:\n",
        "        full_string += f\"{row['Pergunta']} {row['Resposta']} \"\n",
        "        first_line = False\n",
        "        continue\n",
        "      if pd.notna(row['Pergunta']):\n",
        "        full_string += f\"{row['Pergunta']} \"\n",
        "      df.loc[index, 'Pergunta'] = full_string\n",
        "      if pd.notna(row['Resposta']):\n",
        "        full_string += f\"{row['Resposta']} \"\n",
        "\n",
        "\n",
        "  mapping = {old_value: i + no_start for i, old_value in enumerate(unique_numbers)}\n",
        "  df['No'] = df['No'].map(mapping)\n",
        "\n",
        "  return df\n",
        "\n",
        "conv2_df = combine_questions_and_answers(conv2_df, len(combined_qa_df) + 1)\n",
        "conv1_df = combine_questions_and_answers(conv1_df, len(conv2_df) + len(combined_qa_df) + 1)"
      ],
      "metadata": {
        "id": "1g1ubWrwFbXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([combined_qa_df, conv2_df], ignore_index=True)\n",
        "combined_df = pd.concat([combined_df, conv1_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "x2Ou9FOj4W5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "substituicoes = dict(zip(dictionary_df['Palavra '], dictionary_df['Significado']))\n",
        "\n",
        "def substituir_palavras(texto, substituicoes):\n",
        "    \"\"\"\n",
        "    Substitui palavras em um texto com base no dicionário mandado pelo Inácio\n",
        "    \"\"\"\n",
        "    for palavra, significado in substituicoes.items():\n",
        "        texto = texto.replace(palavra, significado)\n",
        "    return texto\n",
        "\n",
        "combined_df['Pergunta'] = combined_df['Pergunta'].apply(lambda x: substituir_palavras(x, substituicoes))"
      ],
      "metadata": {
        "id": "lIgWBbJwNEpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df"
      ],
      "metadata": {
        "id": "EEiI2vH9NO4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df.to_csv('/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/data_sprint_3.csv', index=False)"
      ],
      "metadata": {
        "id": "xUmRpJi_aZJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}