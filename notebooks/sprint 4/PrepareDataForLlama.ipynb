{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "2qVy9dn7RrqX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "NStX89Ri7bz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6740294-ed0b-4688-e546-d6628ebd9619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import unittest\n",
        "import json\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leitura e combina√ß√£o dos datasets"
      ],
      "metadata": {
        "id": "HYgDOKDVSMXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fun√ß√µes para fazer um tratamento prim√°rio dos dados"
      ],
      "metadata": {
        "id": "61wZG6EiRura"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_emojis(text):\n",
        "    \"\"\"\n",
        "    Remove emojis de um texto.\n",
        "\n",
        "    Args:\n",
        "        text (str): O texto a ser processado.\n",
        "\n",
        "    Returns:\n",
        "        str: O texto sem emojis.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        emoji_pattern = re.compile(\n",
        "            \"[\"\n",
        "            \"\\U0001F600-\\U0001F64F\"\n",
        "            \"\\U0001F300-\\U0001F5FF\"\n",
        "            \"\\U0001F680-\\U0001F6FF\"\n",
        "            \"\\U0001F1E0-\\U0001F1FF\"\n",
        "            \"\\U00002500-\\U00002BEF\"\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U00002702-\\U000027B0\"\n",
        "            \"\\U000024C2-\\U0001F251\"\n",
        "            \"\\U0001f926-\\U0001f937\"\n",
        "            \"\\U00010000-\\U0010ffff\"\n",
        "            \"\\u200d\"\n",
        "            \"\\u2640-\\u2642\"\n",
        "            \"\\u2600-\\u2B55\"\n",
        "            \"\\u23cf\"\n",
        "            \"\\u23e9\"\n",
        "            \"\\u231a\"\n",
        "            \"\\u3030\"\n",
        "            \"\\ufe0f\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def standardize_columns(df):\n",
        "  \"\"\"\n",
        "  Padroniza as colunas de um DataFrame.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com as colunas padronizadas.\n",
        "  \"\"\"\n",
        "  if 'Resposta\\n' in df.columns:\n",
        "    df = df.rename(columns={'Resposta\\n': 'Resposta'})\n",
        "  if 'Unnamed: 0' in df.columns:\n",
        "    df = df.rename(columns={'Unnamed: 0': 'No'})\n",
        "  if 'Comentario do Inacio' in df.columns:\n",
        "    df = df.drop(columns=['Comentario do Inacio'])\n",
        "\n",
        "  df = df.dropna(subset=['Pergunta', 'Resposta'], how='all')\n",
        "  df = df.replace('\\n', ' ', regex=True)\n",
        "\n",
        "  df['Pergunta'] = df['Pergunta'].apply(remove_emojis)\n",
        "  df['Resposta'] = df['Resposta'].apply(remove_emojis)\n",
        "  df['No'] = df['No'].fillna(method='ffill')\n",
        "  df['Intencao'] = df['Intencao'].fillna(method='ffill')\n",
        "\n",
        "  return df\n",
        "\n",
        "def organize_no(df, no_start, unique_numbers):\n",
        "  \"\"\"\n",
        "  Organiza os n√∫meros de conversa a partir do √∫ltimo n√∫mero do csv anterior\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "      no_start (int): O n√∫mero inicial para o No (n√∫mero do chat).\n",
        "      unique_numbers (list): A lista de n√∫meros √∫nicos.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com os n√∫meros organizados.\n",
        "  \"\"\"\n",
        "  mapping = {old_value: i + no_start for i, old_value in enumerate(unique_numbers)}\n",
        "  df['No'] = df['No'].map(mapping)\n",
        "  return df"
      ],
      "metadata": {
        "id": "Afid_YDLx9GO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tratamento prim√°rio e concatena√ß√£o dos datasets"
      ],
      "metadata": {
        "id": "sS1UxRhoSDtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv1 = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Conversas1.csv\"\n",
        "conv2 = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Conversas2.csv\"\n",
        "dictionary = \"/content/drive/Shareddrives/grupo3moshi/SPRINT_3/data/Dicionario.csv\"\n",
        "\n",
        "\n",
        "# Padroniza as colunas para depois conseguir concatenar todos os dataframes\n",
        "conv1_df = standardize_columns(pd.read_csv(conv1))\n",
        "conv2_df = standardize_columns(pd.read_csv(conv2))\n",
        "dictionary_df = pd.read_csv(dictionary)\n",
        "\n",
        "unique_numbers = conv1_df['No'].unique()\n",
        "no_start = conv1_df['No'].values[-1:][0]\n",
        "\n",
        "conv2_df = organize_no(conv2_df, no_start + 1, unique_numbers)\n",
        "\n",
        "combined_df = pd.concat([conv1_df, conv2_df], ignore_index=True)\n",
        "combined_df.dropna(inplace = True)\n",
        "\n",
        "combined_df"
      ],
      "metadata": {
        "id": "3itpHm3SGxm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation"
      ],
      "metadata": {
        "id": "GBygo7hGShEI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fun√ß√µes de data augmentation"
      ],
      "metadata": {
        "id": "T8Xw1_IQSkC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_words(sentence):\n",
        "    \"\"\"\n",
        "    Embaralha as palavras em uma frase\n",
        "\n",
        "    Args:\n",
        "      sentence (str): A frase a ser embaralhada.\n",
        "\n",
        "    Returns:\n",
        "      str: A frase embaralhada.\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    random.shuffle(words)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def get_synonyms(word):\n",
        "    \"\"\"\n",
        "    Retorna os sin√¥nimos de uma palavra\n",
        "\n",
        "    Args:\n",
        "      word (str): A palavra a ser pesquisada os sin√¥nimos dela.\n",
        "\n",
        "    Returns:\n",
        "      list: A lista de sin√¥nimos.\n",
        "    \"\"\"\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word, lang='por'):\n",
        "        for lemma in syn.lemmas(lang='por'):\n",
        "            synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
        "    if word in synonyms:\n",
        "        synonyms.remove(word)\n",
        "    return list(synonyms)\n",
        "\n",
        "def synonym_replacement(sentence, n=2):\n",
        "    \"\"\"\n",
        "    Substitui aleatoriamente n palavras por seus sin√¥nimos\n",
        "\n",
        "    Args:\n",
        "      sentence (str): A frase a ter palavras substitu√≠das por sin√¥nimos.\n",
        "      n (int): O n√∫mero de palavras a serem substitu√≠das.\n",
        "\n",
        "    Returns:\n",
        "      str: A frase com sin√¥nimos substitu√≠dos.\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if len(get_synonyms(word)) > 0]))\n",
        "    random.shuffle(random_word_list)\n",
        "\n",
        "    num_replaced = 0\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = get_synonyms(random_word)\n",
        "        if len(synonyms) >= 1:\n",
        "            synonym = random.choice(synonyms)\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "            num_replaced += 1\n",
        "        if num_replaced >= n:\n",
        "            break\n",
        "\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_deletion(words, p=0.14):\n",
        "    \"\"\"\n",
        "    Remove com a probabilidade p as palavras de uma frase\n",
        "\n",
        "    Args:\n",
        "      words (str): A frase a ter palavras removidas.\n",
        "      p (float): A probabilidade de remo√ß√£o de palavras.\n",
        "\n",
        "    Returns:\n",
        "      str: A frase com palavras removidas.\n",
        "    \"\"\"\n",
        "    words = words.split()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        r = random.uniform(0, 1)\n",
        "        if r > p:\n",
        "            new_words.append(word)\n",
        "    return ' '.join(new_words)"
      ],
      "metadata": {
        "id": "Zu8xprboE8Eh"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline de data augmentation"
      ],
      "metadata": {
        "id": "1GKrMYx-SnPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(df):\n",
        "  \"\"\"\n",
        "  Aplica as t√©cnicas de data augmentation ao DataFrame.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com data augmentation aplicada.\n",
        "  \"\"\"\n",
        "\n",
        "  unique_numbers = df['No'].unique()\n",
        "  no_start = conv2_df['No'].values[-1:][0]\n",
        "\n",
        "  df_shuffled = pd.DataFrame()\n",
        "  df_shuffled = df.copy()\n",
        "  # Aplicando o embaralhamento de palavras\n",
        "  df_shuffled['Pergunta'] = df['Pergunta'].apply(lambda x: shuffle_words(x))\n",
        "  df_shuffled['Resposta'] = df['Resposta'].apply(lambda x: shuffle_words(x))\n",
        "  df_shuffled = organize_no(df_shuffled, no_start + 1, unique_numbers)\n",
        "\n",
        "  df_synonym = pd.DataFrame()\n",
        "  df_synonym = df.copy()\n",
        "  # Aplicando a substitui√ß√£o de sin√¥nimos\n",
        "  df_synonym['Pergunta'] = df['Pergunta'].apply(lambda x: synonym_replacement(x))\n",
        "  df_synonym['Resposta'] = df['Resposta'].apply(lambda x: synonym_replacement(x))\n",
        "  df_synonym = organize_no(df_synonym, no_start * 2 + 1, unique_numbers)\n",
        "\n",
        "  df_deletion = pd.DataFrame()\n",
        "  df_deletion = df.copy()\n",
        "  # Aplicando a dele√ß√£o aleat√≥ria\n",
        "  df_deletion['Pergunta'] = df['Pergunta'].apply(lambda x: random_deletion(x))\n",
        "  df_deletion['Resposta'] = df['Resposta'].apply(lambda x: random_deletion(x))\n",
        "  df_deletion = organize_no(df_deletion, no_start * 3 + 1, unique_numbers)\n",
        "\n",
        "  augmented_df = pd.concat([df, df_shuffled, df_synonym, df_deletion], ignore_index=True)\n",
        "  return augmented_df\n",
        "\n",
        "augmented_df = augment_data(combined_df)\n",
        "augmented_df"
      ],
      "metadata": {
        "id": "4I_ukUrmEmSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Substitui√ß√£o de palavras japonesas"
      ],
      "metadata": {
        "id": "_0Lyb9LpThxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "substituicoes = dict(zip(dictionary_df['Palavra '], dictionary_df['Significado']))\n",
        "\n",
        "def substituir_palavras(texto, substituicoes):\n",
        "    \"\"\"\n",
        "    Substitui palavras em um texto com base no dicion√°rio mandado pelo In√°cio\n",
        "\n",
        "    Args:\n",
        "      texto (str): O texto a ser processado.\n",
        "      substituicoes (dict): O dicion√°rio de substitui√ß√µes.\n",
        "\n",
        "    Returns:\n",
        "      str: O texto com palavras substitu√≠das.\n",
        "    \"\"\"\n",
        "    for palavra, significado in substituicoes.items():\n",
        "        texto = texto.replace(palavra, significado)\n",
        "    return texto\n",
        "\n",
        "augmented_df['Pergunta'] = augmented_df['Pergunta'].apply(lambda x: substituir_palavras(x, substituicoes))"
      ],
      "metadata": {
        "id": "lIgWBbJwNEpu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepara√ß√£o da tabela para treinamento do modelo"
      ],
      "metadata": {
        "id": "juQevPeuTpqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_questions_and_answers(df):\n",
        "  \"\"\"\n",
        "  Combina as respostas e perguntas em uma √∫nica coluna, de uma forma que tenha\n",
        "  o hist√≥rico do chat at√© aquele momento.\n",
        "\n",
        "  Args:\n",
        "      df (pd.DataFrame): O DataFrame a ser processado.\n",
        "\n",
        "  Returns:\n",
        "      pd.DataFrame: O DataFrame com as perguntas e respostas combinadas.\n",
        "  \"\"\"\n",
        "  unique_numbers = df['No'].unique()\n",
        "\n",
        "  # Para cada chat, junta as perguntas e respostas, criando um hist√≥rico da\n",
        "  # conversa at√© aquela pergunta\n",
        "  for number in unique_numbers:\n",
        "    full_string = \"\"\n",
        "    first_line = True\n",
        "    rows = df.loc[df['No'] == number, ['Pergunta', 'Resposta']]\n",
        "    for index, row in rows.iterrows():\n",
        "      if first_line:\n",
        "        full_string += f\"{row['Pergunta']} {row['Resposta']} \"\n",
        "        first_line = False\n",
        "        continue\n",
        "      if pd.notna(row['Pergunta']):\n",
        "        full_string += f\"{row['Pergunta']} \"\n",
        "      df.loc[index, 'Pergunta'] = full_string\n",
        "      if pd.notna(row['Resposta']):\n",
        "        full_string += f\"{row['Resposta']} \"\n",
        "\n",
        "  return df\n",
        "\n",
        "return_df = combine_questions_and_answers(augmented_df)\n",
        "\n",
        "return_df"
      ],
      "metadata": {
        "id": "1g1ubWrwFbXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testes"
      ],
      "metadata": {
        "id": "PftQvfaNPMLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste fun√ß√µes de tratamento prim√°rio"
      ],
      "metadata": {
        "id": "2LMLBcQ6TKcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestPrimaryTreatmentFunctions(unittest.TestCase):\n",
        "\n",
        "    def test_remove_emojis(self):\n",
        "        self.assertEqual(remove_emojis(\"Ol√° üòÉ!\"), \"Ol√° !\")\n",
        "        self.assertEqual(remove_emojis(\"üöÄ Vamos para Marte!\"), \" Vamos para Marte!\")\n",
        "        self.assertEqual(remove_emojis(\"Sem emojis aqui.\"), \"Sem emojis aqui.\")\n",
        "\n",
        "        self.assertEqual(remove_emojis(\"\"), \"\")\n",
        "\n",
        "        self.assertEqual(remove_emojis(\"Texto sem emojis\"), \"Texto sem emojis\")\n",
        "\n",
        "        self.assertEqual(remove_emojis(\"12345 @$%*!\"), \"12345 @$%*!\")\n",
        "\n",
        "        self.assertEqual(remove_emojis(12345), 12345)\n",
        "\n",
        "    def test_standardize_columns(self):\n",
        "        df = pd.DataFrame({\n",
        "            'Unnamed: 0': [1, 2, 3],\n",
        "            'Pergunta': ['Qual o seu nome?', None, 'De onde voc√™ √©?'],\n",
        "            'Resposta\\n': ['Meu nome √© Pedro', 'Eu sou do Brasil', None],\n",
        "            'Comentario do Inacio': ['irrelevante', 'irrelevante', 'irrelevante'],\n",
        "            'Intencao': ['Saudacao', None, 'Origem']\n",
        "        })\n",
        "\n",
        "        df_padronizado = standardize_columns(df)\n",
        "\n",
        "        self.assertIn('No', df_padronizado.columns)\n",
        "        self.assertIn('Resposta', df_padronizado.columns)\n",
        "        self.assertNotIn('Comentario do Inacio', df_padronizado.columns)\n",
        "\n",
        "        self.assertEqual(df_padronizado['Intencao'].iloc[1], 'Saudacao')\n",
        "\n",
        "        self.assertNotIn('üöÄ', df_padronizado['Pergunta'].iloc[0])\n",
        "        self.assertNotIn('üòÉ', df_padronizado['Resposta'].iloc[0])\n",
        "\n",
        "    def test_organize_no(self):\n",
        "        df = pd.DataFrame({\n",
        "            'No': [10, 10, 20, 20],\n",
        "            'Pergunta': ['Como est√°?', 'Qual o seu nome?', 'De onde voc√™ √©?', 'Qual sua idade?'],\n",
        "            'Resposta': ['Bem', 'Pedro', 'Brasil', '20 anos']\n",
        "        })\n",
        "\n",
        "        unique_numbers = [10, 20]\n",
        "        df_organizado = organize_no(df, no_start=100, unique_numbers=unique_numbers)\n",
        "\n",
        "        self.assertEqual(df_organizado['No'].iloc[0], 100)\n",
        "        self.assertEqual(df_organizado['No'].iloc[2], 101)"
      ],
      "metadata": {
        "id": "nMBbU0r5Meu2"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste fun√ß√µes de data augmentation"
      ],
      "metadata": {
        "id": "OGWpmDglAAIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestTextAugmentation(unittest.TestCase):\n",
        "\n",
        "    def test_shuffle_words(self):\n",
        "        sentence = \"Isto √© um teste\"\n",
        "        shuffled = shuffle_words(sentence)\n",
        "        self.assertNotEqual(shuffled, sentence)\n",
        "        self.assertCountEqual(shuffled.split(), sentence.split())\n",
        "\n",
        "        single_word = \"teste\"\n",
        "        self.assertEqual(shuffle_words(single_word), single_word)\n",
        "\n",
        "        empty_sentence = \"\"\n",
        "        self.assertEqual(shuffle_words(empty_sentence), \"\")\n",
        "\n",
        "    def test_get_synonyms(self):\n",
        "        synonyms = get_synonyms(\"feliz\")\n",
        "        self.assertTrue(isinstance(synonyms, list))\n",
        "        self.assertIn(\"contente\", synonyms)\n",
        "\n",
        "        synonyms_no_result = get_synonyms(\"inexistente\")\n",
        "        self.assertEqual(synonyms_no_result, [])\n",
        "\n",
        "        synonyms_common_word = get_synonyms(\"bom\")\n",
        "        self.assertTrue(len(synonyms_common_word) > 0)\n",
        "\n",
        "    def test_synonym_replacement(self):\n",
        "        random.seed(42)\n",
        "\n",
        "        sentence = \"Este √© um teste simples\"\n",
        "        replaced_sentence = synonym_replacement(sentence, n=1)\n",
        "\n",
        "        self.assertNotEqual(replaced_sentence, sentence)\n",
        "        self.assertEqual(len(replaced_sentence.split()), len(sentence.split()))\n",
        "\n",
        "    def test_random_deletion(self):\n",
        "        random.seed(42)\n",
        "\n",
        "        sentence = \"Este √© um teste de remo√ß√£o\"\n",
        "        deleted_sentence = random_deletion(sentence, p=0.5)\n",
        "\n",
        "        self.assertNotEqual(deleted_sentence, sentence)\n",
        "        self.assertLess(len(deleted_sentence.split()), len(sentence.split()))\n",
        "\n",
        "        no_deletion_sentence = random_deletion(sentence, p=0)\n",
        "        self.assertEqual(no_deletion_sentence, sentence)\n",
        "\n",
        "        full_deletion_sentence = random_deletion(sentence, p=1)\n",
        "        self.assertEqual(full_deletion_sentence, \"\")"
      ],
      "metadata": {
        "id": "IbpIFf7aAD0y"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste pipeline de data augmentation"
      ],
      "metadata": {
        "id": "Qfr6Q0b3DQhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestAugmentData(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.df = pd.DataFrame({\n",
        "            'No': [1, 2],\n",
        "            'Pergunta': ['Como voc√™ est√°?', 'Qual √© o seu nome?'],\n",
        "            'Resposta': ['Estou bem, obrigado!', 'Meu nome √© ChatGPT.'],\n",
        "            'Intencao': ['saudacao', 'identificacao']\n",
        "        })\n",
        "\n",
        "    def test_augment_data(self):\n",
        "\n",
        "        augmented_df = augment_data(self.df)\n",
        "\n",
        "        self.assertEqual(len(augmented_df), len(self.df) * 4)\n",
        "\n",
        "        self.assertTrue(all(col in augmented_df.columns for col in ['No', 'Pergunta', 'Resposta', 'Intencao']))"
      ],
      "metadata": {
        "id": "eNv--BcoDRj5"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste substitui√ß√£o de palavras japonesas"
      ],
      "metadata": {
        "id": "0yJj7BQWP9FU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDictionary(unittest.TestCase):\n",
        "  def test_japanese_word_replacement(self):\n",
        "        sentence = \"Oi kakunin tudo gomen?\"\n",
        "        replaced_sentence = substituir_palavras(sentence, substituicoes)\n",
        "\n",
        "        self.assertNotEqual(replaced_sentence, sentence)\n",
        "        self.assertEqual(replaced_sentence, \"Oi verificacao tudo desculpe?\")"
      ],
      "metadata": {
        "id": "uvRsEt1BQA5l"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste concatena√ß√£o perguntas e respostas"
      ],
      "metadata": {
        "id": "AQ0rH7FYSE25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestCombineQuestionsAndAnswers(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.df = pd.DataFrame({\n",
        "            'No': [1, 1, 1, 2, 2],\n",
        "            'Pergunta': ['Boa tarde Acabei de fazer a transfer√™ncia de 22+23, Total de 45Yenes',\n",
        "                         'Poderia fazer a remessa de 22yenes para o BB RR obrigado E de 23yenes para o AAA MMM caixa econ√¥mica federal Obrigado',\n",
        "                         \"Obrigado Uma boa noite Deus aben√ßoe\", 'Boa noite, fiz uma transfer√™ncia de 30.000 yenes, √© pra LLL NNN . Arigatou', 'Me confirma quanto deu em real? \\\"Bom dia! Me confirma o valor dessa transfer√™ncia de ontem por favor \\\"'],\n",
        "            'Resposta': ['Boa tarde', \"Iremos processar a sua solicitacao. Muito obrigada e otima tarde.\", \"Qualquer d√∫vida estamos √† disposi√ß√£o. Obrigado.\",\n",
        "                         \"Obrigado pela confirma√ß√£o! Vamos processar a sua remessa.\", \"Bom dia. Ir chegar o valor de BRL 960.96 reais. \\\n",
        "                         Perdao, pela nova cotacao, chegara o valor de BRL 965.81. Qualquer d√∫vida estamos √† disposi√ß√£o. Obrigado. Um otimo dia\"],\n",
        "            'Intencao': ['Pedido de envio via metodo \"ByPhone\"', 'Pedido de envio via metodo \"ByPhone\"', 'Pedido de envio via metodo \"ByPhone\"', 'Pedido de envio via metodo \"ByPhone\"',\n",
        "                         'Confirmacao de cambio/taxas']\n",
        "        })\n",
        "\n",
        "    def test_combine_questions_and_answers(self):\n",
        "        combined_df = combine_questions_and_answers(self.df.copy())\n",
        "\n",
        "        expected_history_chat_1 = [\n",
        "            \"Boa tarde Acabei de fazer a transfer√™ncia de 22+23, Total de 45Yenes\",\n",
        "            \"Boa tarde Acabei de fazer a transfer√™ncia de 22+23, Total de 45Yenes Boa tarde Poderia fazer a remessa de 22yenes para o BB RR obrigado E de 23yenes para o AAA MMM caixa econ√¥mica federal Obrigado \",\n",
        "            \"Boa tarde Acabei de fazer a transfer√™ncia de 22+23, Total de 45Yenes Boa tarde Poderia fazer a remessa de 22yenes para o BB RR obrigado E de 23yenes para o AAA MMM caixa econ√¥mica federal Obrigado Iremos processar a sua solicitacao. Muito obrigada e otima tarde. Obrigado Uma boa noite Deus aben√ßoe \"\n",
        "        ]\n",
        "\n",
        "        for i, expected in enumerate(expected_history_chat_1):\n",
        "            self.assertEqual(combined_df.loc[i, 'Pergunta'], expected)\n",
        "\n",
        "        expected_history_chat_2 = [\n",
        "            \"Boa noite, fiz uma transfer√™ncia de 30.000 yenes, √© pra LLL NNN . Arigatou\",\n",
        "            \"Boa noite, fiz uma transfer√™ncia de 30.000 yenes, √© pra LLL NNN . Arigatou Obrigado pela confirma√ß√£o! Vamos processar a sua remessa. Me confirma quanto deu em real? \\\"Bom dia! Me confirma o valor dessa transfer√™ncia de ontem por favor \\\" \"\n",
        "        ]\n",
        "\n",
        "        for i, expected in enumerate(expected_history_chat_2, start=3):\n",
        "            self.assertEqual(combined_df.loc[i, 'Pergunta'], expected)"
      ],
      "metadata": {
        "id": "l2c4v9FjSKGA"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execu√ß√£o dos testes"
      ],
      "metadata": {
        "id": "xTDAv9LhPudE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqocuZLjPodB",
        "outputId": "02b455a3-a6b6-4018-ccc7-908803930fbb"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".....<ipython-input-57-1231c6b11aad>:61: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['No'] = df['No'].fillna(method='ffill')\n",
            "<ipython-input-57-1231c6b11aad>:62: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df['Intencao'] = df['Intencao'].fillna(method='ffill')\n",
            ".....\n",
            "----------------------------------------------------------------------\n",
            "Ran 10 tests in 0.049s\n",
            "\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exportar csv para o drive"
      ],
      "metadata": {
        "id": "voCk1QyQPRLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "return_df.to_csv('/content/drive/Shareddrives/grupo3moshi/SPRINT_4/augmented_data_sprint_4.csv', index=False)"
      ],
      "metadata": {
        "id": "PluPeY83_3aM"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}